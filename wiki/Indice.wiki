#sidebar MenuLateral
= Introduction =


= Estado actual = 
Migración de la implementación a Rapidminer 4.4
Se realiza el registro de las distancias/similitudes
Migración de las implementaciones realizadas en YALE 3.4 a RapidMiner 4.2


= Problemas actuales =
Revisar la ampliación de los discretizadores para el soporte de las series. Puede ser mejor eliminar el sistema de parámetros y pasarlo a la discretización de todos los atributos que formen una serie de forma conjunta. 

Dependerá del mecanismo que se use para el Kernel Intervalar. Al definir las distancias como clases dentro del paquete tools.math ya no son operadores y por tanto no tienen acceso al conjunto de objetos que se están procesando. Por tanto la información de los límites deberá llegar desde otro lugar. Además la instanciación de la distancia se realiza por parte del operador que la use (p.ej. NearestNeighbors) por lo que no se le puede completar información de forma previa.

----

==Estructura del plugin==
=== qbts.distances ===
  * Se definen aquí todas las distancias, que heredan de !AbstractExtendedRealValueBasedSimilarity y el operador que genera la similitud denominado QBSimilaritySetup

=== qbts.preprocessing.discretization ===
  * Contiene los discretizadores de RM ampliados a la aplicación de series. Finalmente se opta por añadir a los incluidos en RM4.1 un nuevo parámetro que indique que todos los atributos se discreticen juntos. La opción de añadir también el procesado de series (con deteccion de los atributos de principio y fin) se desecha por ser más compleja por tener que contemplar más situaciones por tener dos parámetros relacionados, es más intrusivo en el código de RM y además incluir la opción seleccionada.
  * También existe un !DiscretizationModelSeries que es una herencia directa y casi vacía del !DiscretizationModel de RM que se crea sólo por que los constructores de !DiscretizationModel son privados y por tanto debe estar en el mismo paquete en que sea accedido.
  * Ver [Discretizacion Explicacion de funcionamiento]
~~qbts.discretization~~

 * ~~yale.operator.learner.lazy Incluye código copiado de RM y ligeramente modificado. Incluye un nuevo KNNLearner y !SimiliarityUtil, ver [KNNLearnerSimilarity razonamiento]~~


----

El Kernel intervalar depende de los valores límite de los intervalos de discretización
(tanto el menor como el mayor) pero en el caso de las discretizaciones los valores
mínimos y máximo de los rangos menor y mayor respectivamente son almacenados como 
-infinito y +infinito (o ni siquiera son almacenados).

Para obtener esos valores requeriría accceder al ExampleSet original y obtenerlos de
las estadísticas de los propios atributos. Pero se supone que el conjunto que llega aquí 
ya está discretizado por lo que eso se ha perdido. A no ser que se almacene de forma temporal (en el caso de que venga de una validación cruzada) y se recupere posteriormente.

Las soluciones pueden ser:
   * Obligar a la entrada del conjunto original de nuevo.- De todas formas el acceso al conjunto de datos original no es suficiente puesto que si se han discretizado todos los atributos juntos habría que obtener los valores máximos y mínimos de todos, pero desde el modelo de discretización no se sabe como se hizo la discretización, por lo que habría que considerar la colocación de un parámetro para poder comunicarlo.
   * Realizar un nuevo operador que realice la discretización y almacene los valores
   * Utilizar obligatoriamente los límites del rango en los nombres de los valores nominales en que se discretizan los valores reales. Existen tres mecanismos para denominar las clases de una discreización: corto (rangea, rangeb), largo y con rango (que introduce los límites en el nombre
---- 


¿Y si pudiera usar el !AttributeSubsetPreprocessing para hacer las discretizaciones?
Si hay varias series sólo tengo que hacer una entrada por cada grupo de atributos y ya está.
ESTUDIAR:

¿como hacer que el BinDiscretization (por ejemplo) actúe sobre todos? Realmente hay que crear un idéntico pero modificado, ¿que ventaja tiene entonces utilizar el AttributeSubsetPreprocessing? Pero lo que es peor,si tengo que tratar varias veces un conjunto de ejemplos entonces me va a quedar un conjunto de modelos uno para cada serie. Supongo que eso no es problema pero puede ser inconveniente en la selección de los modelos.

LECCIONES:
Se comprueba que los atributos son realmente sustituidos. El orden en que quedan es: 
  * Si solo se procesan algunos, entonces quedan los atributos especiales, los modificados y el resto.
  * en caso de que la expresión regular afecte a todos, entonces quedan los modificados y los especiales.

-----


LECCIONES:
  # Se puede crear un operador que se encargue de crear una similitud en la que almacene el modelo de discretización.
  # Aunque en la lista del inputContainer (que es donde están los objetos que entran a un operador) sólo tenga un modelo éste vendrá  dentro de un ~~!ModelContainer~~ !GroupedModel. El apply() de un ~~!ModelContainer~~ !GroupedModel realiza la aplicación secuencial de todos los modelos que incluya.
La cuestión es ver como afecta cuando además del modelo de discretización exista uno de aprendizaje en el ~~!ModelContainer~~ !GroupedModel.
  # Hay que crear una clase base de la que dependan todas las similitudes por lo que se pueda requerir; en el caso del kernel hay que almacenar los límites del modelo de discretización. La clase debe definirse al nivel de AbstractRealValueBasedSimilarity para evitar la comprobación que se hace en el init de AbstractRealValueBasedSimilarity de que todos los atributos sean numéricos. En nuestro caso tienen que ser nominales.


----

= Detalles =

AmpliaciOn a series

La idea de ampliar la discretización a series se enfoca por medio de hacer nuevos operadores de discretización olvidando la extensión de RM con las listas de bloques.
Estos operadores se limitan (caso unidimensional) a generar el mismo rango para todos los atributos.
Una vez implementados se puede ver la posibilidad de incluir el código en el operador original.



[Comentarios] 



= [FAQ] =